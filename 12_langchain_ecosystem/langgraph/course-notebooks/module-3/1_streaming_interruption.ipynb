{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arshad62/learn-agentic-ai/blob/main/12_langchain_ecosystem/langgraph/course-notebooks/module-3/1_streaming_interruption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
      "metadata": {
        "id": "319adfec-2d0a-49f2-87f9-275c4a32add2"
      },
      "source": [
        "# Streaming\n",
        "\n",
        "## Review\n",
        "\n",
        "In module 2, covered a few ways to customize graph state and memory.\n",
        "\n",
        "We built up to a Chatbot with external memory that can sustain long-running conversations.\n",
        "\n",
        "## Goals\n",
        "\n",
        "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways.\n",
        "\n",
        "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
      "metadata": {
        "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_google_genai langgraph_sdk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
      "metadata": {
        "id": "70d7e41b-c6ba-4e47-b645-6c110bede549"
      },
      "source": [
        "## Streaming\n",
        "\n",
        "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
        "\n",
        "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b430d92-f595-4322-a56e-06de7485daa8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b430d92-f595-4322-a56e-06de7485daa8",
        "outputId": "8a997103-c1e6-4a24-cbb8-acb07ca16190",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GOOGLE_API_KEY=AIzaSyBwDMAZHrZqrpcF6clOkgz4iLrsBw-A0QQ\n",
            "AIzaSyBwDMAZHrZqrpcF6clOkgz4iLrsBw-A0QQ\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "%env GOOGLE_API_KEY = {userdata.get('GEMINI_API_KEY')}\n",
        "\n",
        "import os\n",
        "print(os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9b2a5d0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b2a5d0a",
        "outputId": "e118e4d9-0592-4188-9d91-be7878393ec1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LANGCHAIN_API_KEY=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\n"
          ]
        }
      ],
      "source": [
        "%env LANGCHAIN_API_KEY = {userdata.get('LANGCHAIN_API_KEY')}\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0682fc",
      "metadata": {
        "id": "4d0682fc"
      },
      "source": [
        "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
        "outputId": "588fabcd-76f4-41fd-a1a5-bd42de3e1f1b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "# LLM\n",
        "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\")\n",
        "\n",
        "# State\n",
        "class State(MessagesState):\n",
        "    summary: str\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State, config: RunnableConfig):\n",
        "\n",
        "    # Get summary if it exists\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = model.invoke(messages, config)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State):\n",
        "\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
        "\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State):\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END\n",
        "\n",
        "# Define a new graph\n",
        "workflow: StateGraph = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(summarize_conversation)\n",
        "\n",
        "# Set the entrypoint as conversation\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# Compile\n",
        "memory: MemorySaver = MemorySaver()\n",
        "graph: CompiledStateGraph = workflow.compile(checkpointer=memory)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f847a787-b301-488c-9b58-cba9f389f55d",
      "metadata": {
        "id": "f847a787-b301-488c-9b58-cba9f389f55d"
      },
      "source": [
        "### Streaming full state\n",
        "\n",
        "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
        "\n",
        "`.stream` and `.astream` are sync and async methods for streaming back results.\n",
        "\n",
        "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
        "\n",
        "* `values`: This streams the full state of the graph after each node is called.\n",
        "* `updates`: This streams updates to the state of the graph after each node is called.\n",
        "\n",
        "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
        "\n",
        "Let's look at `stream_mode=\"updates\"`.\n",
        "\n",
        "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
        "\n",
        "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
        "outputId": "5465abf1-5737-4bec-f2b0-bb191700aad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'conversation': {'messages': AIMessage(content='Hi Arshad!  How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e1a2c19a-23a3-4277-a1bc-0c33abddcd73-0', usage_metadata={'input_tokens': 8, 'output_tokens': 13, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}})}}\n"
          ]
        }
      ],
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Start conversation\n",
        "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Arshad\")]}, config, stream_mode=\"updates\"):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
      "metadata": {
        "id": "0c4882e9-07dd-4d70-866b-dfc530418cad"
      },
      "source": [
        "Let's now just print the state update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c859c777-cb12-4682-9108-6b367e597b81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c859c777-cb12-4682-9108-6b367e597b81",
        "outputId": "2effa4c6-f3d7-4102-8b87-2fdb666a30ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Okay, here are a few jokes of different types:\n",
            "\n",
            "**A pun:**  Why don't scientists trust atoms? Because they make up everything!\n",
            "\n",
            "**A one-liner:**  I used to hate facial hair...but then it grew on me.\n",
            "\n",
            "**A slightly longer joke:**  A man walks into a library and asks for books about paranoia.  The librarian whispers, \"They're right behind you!\"\n",
            "\n",
            "**A knock-knock joke:** Knock knock.\n",
            "...Who's there?\n",
            "Lettuce.\n",
            "Lettuce who?\n",
            "Lettuce in! It's cold out here!\n",
            "\n",
            "\n",
            "Which type of joke did you like best?  I can tell you more like that one, or try some different kinds!\n"
          ]
        }
      ],
      "source": [
        "# Start conversation\n",
        "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"Tell me some jokes\")]}, config, stream_mode=\"updates\"):\n",
        "    chunk['conversation'][\"messages\"].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583bf219-6358-4d06-ae99-c40f43569fda",
      "metadata": {
        "id": "583bf219-6358-4d06-ae99-c40f43569fda"
      },
      "source": [
        "Now, we can see `stream_mode=\"values\"`.\n",
        "\n",
        "This is the `full state` of the graph after the `conversation` node is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
        "outputId": "7e4ef5a5-6b05-4367-eb44-ad175cbcfa10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm Ali\n",
            "---------------------------------------------------------------------------\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm Ali\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Ali!  It's nice to meet you. How can I help you today?\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Start conversation, again\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# Start conversation\n",
        "input_message = HumanMessage(content=\"hi! I'm Ali\")\n",
        "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    for m in event['messages']:\n",
        "        m.pretty_print()\n",
        "    print(\"---\"*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
      "metadata": {
        "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7"
      },
      "source": [
        "### Streaming tokens\n",
        "\n",
        "We often want to stream more than graph state.\n",
        "\n",
        "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
        "\n",
        "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
        "\n",
        "Each event is a dict with a few keys:\n",
        "\n",
        "* `event`: This is the type of event that is being emitted.\n",
        "* `name`: This is the name of event.\n",
        "* `data`: This is the data associated with the event.\n",
        "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
        "\n",
        "Let's have a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
        "outputId": "9ff57644-95bd-424c-ad09-e752bd3b8157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node: . Type: on_chain_start. Name: LangGraph\n",
            "Node: __start__. Type: on_chain_start. Name: __start__\n",
            "Node: __start__. Type: on_chain_start. Name: _write\n",
            "Node: __start__. Type: on_chain_end. Name: _write\n",
            "Node: __start__. Type: on_chain_start. Name: _write\n",
            "Node: __start__. Type: on_chain_end. Name: _write\n",
            "Node: __start__. Type: on_chain_stream. Name: __start__\n",
            "Node: __start__. Type: on_chain_end. Name: __start__\n",
            "Node: conversation. Type: on_chain_start. Name: conversation\n",
            "Node: conversation. Type: on_chat_model_start. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chat_model_end. Name: ChatGoogleGenerativeAI\n",
            "Node: conversation. Type: on_chain_start. Name: _write\n",
            "Node: conversation. Type: on_chain_end. Name: _write\n",
            "Node: conversation. Type: on_chain_start. Name: should_continue\n",
            "Node: conversation. Type: on_chain_end. Name: should_continue\n",
            "Node: conversation. Type: on_chain_stream. Name: conversation\n",
            "Node: conversation. Type: on_chain_end. Name: conversation\n",
            "Node: . Type: on_chain_stream. Name: LangGraph\n",
            "Node: . Type: on_chain_end. Name: LangGraph\n"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
        "\n",
        "input_message = HumanMessage(content=\"Tell me about the Pakistan Cricket Team\")\n",
        "\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "\n",
        "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
      "metadata": {
        "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d"
      },
      "source": [
        "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
        "\n",
        "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
        "\n",
        "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
        "outputId": "95fce2b7-823b-4196-db22-a447e81aca02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 8, 'output_tokens': 0, 'total_tokens': 8, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=' Pakistan cricket team is one of the most passionate and historically significant teams in the world', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=\".  Known for their flair, aggressive batting, and unpredictable nature, they'\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=\"ve experienced both exhilarating highs and crushing lows throughout their history.\\n\\nHere's a breakdown of key aspects:\\n\\n**History and Achievements:**\\n\\n* **Early\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=' Success:** Pakistan quickly established themselves as a force in international cricket after gaining independence in 1947.  They won the Cricket World Cup in 1', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=\"992, a momentous victory led by Imran Khan.\\n* **Ups and Downs:**  Since then, Pakistan has had periods of dominance interspersed with inconsistent performances.  They've consistently produced world-class players but have also faced\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=\" challenges with internal politics, security concerns, and inconsistent team selection.\\n* **Notable Achievements:** Besides the 1992 World Cup win, they've also won the Champions Trophy twice (2009 and 20\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=\"17) and have consistently been competitive in other major tournaments.  They've also had periods of dominance in Test cricket, particularly in the 1980s and early 2000s.\\n\\n**Current Status:**\\n\\n* **A Team in Transition:**  Currently, the Pakistan team is\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=' undergoing a period of transition, aiming to blend experienced players with exciting young talent.  They show flashes of brilliance but consistency remains a challenge.\\n* **Strengths:**  Pakistan possesses a potent fast bowling attack, often considered one of the best in the world.  Their batting can be explosive, but consistency in the middle', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=\" order is an ongoing area for improvement.\\n* **Weaknesses:**  Inconsistency in batting and fielding, along with occasional lapses in team strategy, have often been cited as areas needing improvement.\\n\\n**Key Players (this is subject to change frequently):**\\n\\nThe Pakistan team's composition changes frequently. Currently, key\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=\" players often include prominent batsmen, all-rounders and fast bowlers.  It's best to check current news and team sheets for the most up-to-date information on the playing XI.\\n\\n**Playing Style:**\\n\\nPakistan is known for its aggressive batting approach, especially in limited-overs formats.  \", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content='They often employ a fearless and attacking style, which can lead to spectacular wins but also to equally dramatic collapses.  Their fast bowling attack is a major strength, often troubling even the best batting lineups.\\n\\n**Fanbase:**\\n\\nPakistan cricket enjoys a massive and incredibly passionate fanbase, both domestically and internationally.  Their', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=' matches are always high-energy affairs, with fans known for their unwavering support and enthusiasm.\\n\\nIn short, the Pakistan cricket team is a complex and fascinating entity.  A team with immense talent and potential, they are always capable of surprising even the most seasoned observers.  Their journey is one of highs and lows,', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}})}\n",
            "{'chunk': AIMessageChunk(content=' making them a perpetually compelling team to follow.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': []}, id='run-99382b86-9d88-4ca4-aa90-373d115ccd0e', usage_metadata={'input_tokens': 0, 'output_tokens': 587, 'total_tokens': 587, 'input_token_details': {'cache_read': 0}})}\n"
          ]
        }
      ],
      "source": [
        "node_to_stream = 'conversation'\n",
        "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the Pakistan Cricket Team\")\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    # Get chat model tokens from a particular node\n",
        "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
        "        print(event[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
      "metadata": {
        "id": "226e569a-76c3-43d8-8f89-3ae687efde1c"
      },
      "source": [
        "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
        "outputId": "640467fb-ce77-4670-ff13-58199cfce728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The| Pakistan cricket team is one of the most passionate and historically successful teams in the world|, despite facing significant challenges over the years.  Here's a summary:|\n",
            "\n",
            "**Historical Success:**\n",
            "\n",
            "* **World Cup Winners (1992):**  Their most significant achievement, a triumph led by Imran Khan, showcasing| incredible team spirit and overcoming adversity.\n",
            "* **Champions Trophy Winners (1992, 2017):**  Two victories in this prestigious| tournament, highlighting their ability to perform on the big stage.\n",
            "* **Numerous Bilateral Series Wins:**  Pakistan has consistently produced talented players who have won numerous series against top teams throughout their history.\n",
            "\n",
            "**Playing Style:**\n",
            "\n",
            "* **Known| for aggressive batting and skillful bowling:** Historically, Pakistan has boasted explosive batsmen and cunning bowlers, often capable of match-winning performances.  Their bowling attack, particularly their fast bowlers, has often been a major strength.\n",
            "* **Historically| unpredictable:**  While capable of beating any team on their day, inconsistency has been a recurring theme.  They can produce stunning victories followed by disappointing losses. This unpredictability is part of their mystique and a source of both excitement and frustration for their fans.\n",
            "\n",
            "**Current Status:**\n",
            "\n",
            "* **A team in transition:**|  Like many teams, Pakistan is constantly evolving, with a mix of experienced players and promising youngsters.  Their performance fluctuates depending on the form of key players and the overall team cohesion.\n",
            "* **Strong fast bowling attack:**  This continues to be a significant strength, with several talented fast bowlers regularly featuring in the| team.\n",
            "* **Developing batting line-up:** The batting order is often inconsistent, a key area that requires consistent improvement.\n",
            "\n",
            "**Challenges:**\n",
            "\n",
            "* **Domestic Structure:** The domestic cricket structure in Pakistan has faced challenges in recent years, impacting the development of young talent.\n",
            "* **Security Concerns:**  Security| concerns have historically affected the team's ability to host international matches on home soil.\n",
            "* **Consistency:** The team's inconsistency remains a major hurdle to overcome to achieve sustained success at the highest level.\n",
            "\n",
            "**Overall:**\n",
            "\n",
            "The Pakistan cricket team is a force to be reckoned with, possessing immense talent and a| passionate fanbase.  While inconsistency remains a challenge, their history is filled with moments of brilliance and their potential remains high.  Their matches are always exciting, and their future success will depend on addressing their structural challenges and maintaining a consistent level of performance.|"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the Pakistan Cricket Team\")\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    # Get chat model tokens from a particular node\n",
        "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
        "        data = event[\"data\"]\n",
        "        print(data[\"chunk\"].content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
      "metadata": {
        "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db"
      },
      "source": [
        "### Streaming with LangGraph API\n",
        "\n",
        "--\n",
        "\n",
        "** DISCLAIMER**\n",
        "\n",
        "*Running Studio currently requires a Mac. If you are not using a Mac, then skip this step.*\n",
        "\n",
        "*Also, if you are running this notebook in CoLab, then skip this step.*\n",
        "\n",
        "--\n",
        "\n",
        "The LangGraph API [has first class support for streaming](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#streaming).\n",
        "\n",
        "Let's load our `agent` in the Studio UI, which uses `module-3/studio/agent.py` set in `module-3/studio/langgraph.json`.\n",
        "\n",
        "The LangGraph API serves as the back-end for Studio.\n",
        "\n",
        "We can interact directly with the LangGraph API via the LangGraph SDK.\n",
        "\n",
        "We just need to get the URL for the local deployment from Studio.\n",
        "\n",
        "![Screenshot 2024-08-27 at 2.20.34 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf8943c3d4df239cbf0f_streaming2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "079c2ad6",
      "metadata": {
        "id": "079c2ad6"
      },
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client\n",
        "\n",
        "# Replace this with the URL of your own deployed graph\n",
        "URL = \"https://fe5d-185-200-235-2.ngrok-free.app/\"\n",
        "client = get_client(url=URL)\n",
        "\n",
        "# Search all hosted graphs\n",
        "assistants = await client.assistants.search()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
      "metadata": {
        "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32"
      },
      "source": [
        "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
      "metadata": {
        "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
        "outputId": "b81d81d1-b805-4e3a-d964-785a7961daad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StreamPart(event='metadata', data={'run_id': '1efccd54-7f59-6e34-9c2c-da50267e7a8e', 'attempt': 1})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 8 and 4', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3fa06b6d-d17e-4ce1-9b7d-b14a55165ba0', 'example': False}]})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 8 and 4', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3fa06b6d-d17e-4ce1-9b7d-b14a55165ba0', 'example': False}, {'content': '', 'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 8.0, \"b\": 4.0}'}}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-d3ec93ca-b5f9-44dc-9819-46b3ffc57382-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 8.0, 'b': 4.0}, 'id': '35a35145-465c-4490-802a-2223817ed2e4', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 168, 'output_tokens': 3, 'total_tokens': 171, 'input_token_details': {'cache_read': 0}}}]})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 8 and 4', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3fa06b6d-d17e-4ce1-9b7d-b14a55165ba0', 'example': False}, {'content': '', 'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 8.0, \"b\": 4.0}'}}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-d3ec93ca-b5f9-44dc-9819-46b3ffc57382-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 8.0, 'b': 4.0}, 'id': '35a35145-465c-4490-802a-2223817ed2e4', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 168, 'output_tokens': 3, 'total_tokens': 171, 'input_token_details': {'cache_read': 0}}}, {'content': '32', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '2f7d8189-8cbc-4551-bd38-491e2342b9d7', 'tool_call_id': '35a35145-465c-4490-802a-2223817ed2e4', 'artifact': None, 'status': 'success'}]})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 8 and 4', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3fa06b6d-d17e-4ce1-9b7d-b14a55165ba0', 'example': False}, {'content': '', 'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 8.0, \"b\": 4.0}'}}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-d3ec93ca-b5f9-44dc-9819-46b3ffc57382-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 8.0, 'b': 4.0}, 'id': '35a35145-465c-4490-802a-2223817ed2e4', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 168, 'output_tokens': 3, 'total_tokens': 171, 'input_token_details': {'cache_read': 0}}}, {'content': '32', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '2f7d8189-8cbc-4551-bd38-491e2342b9d7', 'tool_call_id': '35a35145-465c-4490-802a-2223817ed2e4', 'artifact': None, 'status': 'success'}, {'content': 'The result is 32.', 'additional_kwargs': {}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'type': 'ai', 'name': None, 'id': 'run-26ed30c9-1713-4df0-9488-ee832740e673-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 202, 'output_tokens': 8, 'total_tokens': 210, 'input_token_details': {'cache_read': 0}}}]})\n"
          ]
        }
      ],
      "source": [
        "# Create a new thread\n",
        "thread = await client.threads.create()\n",
        "# Input message\n",
        "input_message = HumanMessage(content=\"Multiply 8 and 4\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      assistant_id=\"agent\",\n",
        "                                      input={\"messages\": [input_message]},\n",
        "                                      stream_mode=\"values\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
      "metadata": {
        "id": "556dc7fd-1cae-404f-816a-f13d772b3b14"
      },
      "source": [
        "The streamed objects have:\n",
        "\n",
        "* `event`: Type\n",
        "* `data`: State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "57b735aa-139c-45a3-a850-63519c0004f0",
      "metadata": {
        "id": "57b735aa-139c-45a3-a850-63519c0004f0",
        "outputId": "bd8e1957-a1f9-4f7b-b603-d00fb396d851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================\n",
            "content='Multiply 2 and 3' additional_kwargs={'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'example': False} response_metadata={} id='4d9e4f0f-5ddd-4ac7-84af-341ef8270915'\n",
            "=========================\n",
            "content='' additional_kwargs={'additional_kwargs': {'function_call': {'name': 'multiply', 'arguments': '{\"a\": 2.0, \"b\": 3.0}'}}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 168, 'output_tokens': 3, 'total_tokens': 171, 'input_token_details': {'cache_read': 0}}} response_metadata={} id='run-2ae47bf3-e2ae-4495-826c-d688c004321d-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2.0, 'b': 3.0}, 'id': '249961e8-b7b4-4015-b1e6-ec44525d5039', 'type': 'tool_call'}]\n",
            "=========================\n",
            "content='6' name='multiply' id='9b53e530-432b-4cf9-a8ab-a3384a27da56' tool_call_id='249961e8-b7b4-4015-b1e6-ec44525d5039'\n",
            "=========================\n",
            "content='The result is 6.' additional_kwargs={'additional_kwargs': {}, 'response_metadata': {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 201, 'output_tokens': 7, 'total_tokens': 208, 'input_token_details': {'cache_read': 0}}} response_metadata={} id='run-933249f0-18ce-4959-8bb0-318236b20fa5-0'\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import convert_to_messages\n",
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
        "    messages = event.data.get('messages',None)\n",
        "    if messages:\n",
        "        print(convert_to_messages(messages)[-1])\n",
        "    print('='*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a555d186-27be-4ddf-934c-895a3105035d",
      "metadata": {
        "id": "a555d186-27be-4ddf-934c-895a3105035d"
      },
      "source": [
        "There are some new streaming mode that are only supported via the API.\n",
        "\n",
        "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
        "\n",
        "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
        "\n",
        "All events emitted using `messages` mode have two attributes:\n",
        "\n",
        "* `event`: This is the name of the event\n",
        "* `data`: This is data associated with the event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
      "metadata": {
        "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
        "outputId": "a1ec92ac-0448-4b90-b391-2d7387eb4020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metadata\n",
            "messages/metadata\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/metadata\n",
            "messages/complete\n",
            "messages/metadata\n",
            "messages/partial\n",
            "messages/partial\n"
          ]
        }
      ],
      "source": [
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      assistant_id=\"agent\",\n",
        "                                      input={\"messages\": [input_message]},\n",
        "                                      stream_mode=\"messages\"):\n",
        "    print(event.event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
      "metadata": {
        "id": "8de2f1ea-b232-43fc-af7a-320efce83381"
      },
      "source": [
        "We can see a few events:\n",
        "\n",
        "* `metadata`: metadata about the run\n",
        "* `messages/complete`: fully formed message\n",
        "* `messages/partial`: chat model tokens\n",
        "\n",
        "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
        "\n",
        "Now, let's show how to stream these messages.\n",
        "\n",
        "We'll define a helper function for better formatting of the tool calls in messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
      "metadata": {
        "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
        "outputId": "c25a4aec-a63d-4527-f11b-c408fa9039a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: Run ID - 1efccd59-a569-6a2b-8dfc-d34e7d4cce44\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: 43ba3eec-38d9-4ccf-9b3c-1ec203d19b84, Function: multiply, Arguments: {'a': 8.0, 'b': 4.0}\n",
            "Response Metadata: Finish Reason - N/A\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: 43ba3eec-38d9-4ccf-9b3c-1ec203d19b84, Function: multiply, Arguments: {'a': 8.0, 'b': 4.0}\n",
            "Response Metadata: Finish Reason - STOP\n",
            "--------------------------------------------------\n",
            "AI: The\n",
            "Response Metadata: Finish Reason - N/A\n",
            "--------------------------------------------------\n",
            "AI: The result is 32.\n",
            "Response Metadata: Finish Reason - STOP\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 8 and 4\")\n",
        "\n",
        "def format_tool_calls(tool_calls):\n",
        "    \"\"\"\n",
        "    Format a list of tool calls into a readable string.\n",
        "\n",
        "    Args:\n",
        "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
        "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if tool_calls:\n",
        "        formatted_calls = []\n",
        "        for call in tool_calls:\n",
        "            formatted_calls.append(\n",
        "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
        "            )\n",
        "        return \"\\n\".join(formatted_calls)\n",
        "    return \"No tool calls\"\n",
        "\n",
        "async for event in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"agent\",\n",
        "    input={\"messages\": [input_message]},\n",
        "    stream_mode=\"messages\",):\n",
        "\n",
        "    # Handle metadata events\n",
        "    if event.event == \"metadata\":\n",
        "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Handle partial message events\n",
        "    elif event.event == \"messages/partial\":\n",
        "        for data_item in event.data:\n",
        "            # Process user messages\n",
        "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
        "                print(f\"Human: {data_item['content']}\")\n",
        "            else:\n",
        "                # Extract relevant data from the event\n",
        "                tool_calls = data_item.get(\"tool_calls\", [])\n",
        "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
        "                content = data_item.get(\"content\", \"\")\n",
        "                response_metadata = data_item.get(\"response_metadata\", {})\n",
        "\n",
        "                if content:\n",
        "                    print(f\"AI: {content}\")\n",
        "\n",
        "                if tool_calls:\n",
        "                    print(\"Tool Calls:\")\n",
        "                    print(format_tool_calls(tool_calls))\n",
        "\n",
        "                if invalid_tool_calls:\n",
        "                    print(\"Invalid Tool Calls:\")\n",
        "                    print(format_tool_calls(invalid_tool_calls))\n",
        "\n",
        "                if response_metadata:\n",
        "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
        "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
        "\n",
        "        print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
      "metadata": {
        "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}